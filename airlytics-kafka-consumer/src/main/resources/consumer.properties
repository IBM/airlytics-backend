
topic = demo_p100_r2
bootstrap.servers = b-4.msk-weather-2.agz80z.c5.kafka.us-east-1.amazonaws.com:9094,b-1.msk-weather-2.agz80z.c5.kafka.us-east-1.amazonaws.com:9094,b-2.msk-weather-2.agz80z.c5.kafka.us-east-1.amazonaws.com:9094
consumer.group = conGroup1

#maximum  number of records to retrieve in a single poll request
max.poll.records = 600

# NONE or SSL
security.protocol = SSL

shards.number = 1000
s3.bucket = kafka-analytics
s3.root.folder = parquet
max.dump.records = 200000

# specifies how often the consumer was to intereact with brokers (to confirm it's alive)
# increase this value when the dump is taking too long
max.poll.interval.ms = 300000

#currently 5 minutes
max.dump.interval.ms = 300000

# number of threads writing the parquet files to S3
write.threads.number = 40

# 16*1024*1024 16M (an integer size in bytes)
parquet.row.group.size = 16777216

db.url=jdbc:postgresql://airlock-postgres-dev1-instance-1.cybteji42m6v.eu-west-1.rds.amazonaws.com/users
db.username=postgres
db.password=